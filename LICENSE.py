# -*- coding: utf-8 -*-
"""Untitled50.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eT6VrYkyd5Iu3p0a3N7bBQQPNLGsDrvS

This code preprocesses a diabetes dataset by performing the following steps:

Mount Google Drive: The Google Drive is mounted to access the dataset stored in it.
Data Loading: The dataset is read into a pandas DataFrame.
Initial Inspection: The shape and structure of the dataset are displayed, and the first few rows are printed for preview.
Removing Duplicates: Any duplicate entries in the dataset are removed to ensure clean data.
Outlier Removal: Outliers in the numeric columns are identified and removed using the Interquartile Range (IQR) method. The columns processed are 'BMI', 'MentHlth', 'PhysHlth', 'GenHlth', 'Age', 'Education', and 'Income'.
Normalization: The numeric columns are scaled using MinMaxScaler to normalize the values between 0 and 1.
Missing Values Check: A check for missing values is performed, and the results are printed.
Save Processed Data: The processed dataset is saved into a new CSV file for further use.
This preprocessing ensures that the data is cleaned, scaled, and ready for machine learning tasks.
"""

import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from google.colab import drive

drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/project_ML22/diabetes_012_health_indicators_BRFSS2015.csv')

print("Dataset shape before processing:", df.shape)
df.info()

print("Preview of the dataset:")
print(df.head())

data = df.drop_duplicates()
print(f"Dataset shape after removing duplicates: {data.shape}")

def remove_outliers_iqr(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]

numeric_columns = ['BMI', 'MentHlth', 'PhysHlth', 'GenHlth', 'Age', 'Education', 'Income']
for col in numeric_columns:
    data = remove_outliers_iqr(data, col)

print(f"Dataset shape after outlier removal: {data.shape}")

scaler = MinMaxScaler()
data.loc[:, numeric_columns] = scaler.fit_transform(data[numeric_columns])

print("Preview of the processed dataset:")
print(data.head())

print("Number of missing values after preprocessing:\n", data.isnull().sum())

data.to_csv('/content/drive/MyDrive/processed_data.csv', index=False)
print("Processed dataset saved successfully!")

"""This code performs feature selection and analysis on a dataset using three different techniques: Random Forest, Lasso, and Recursive Feature Elimination (RFE). Here's a breakdown of the steps:

Data Loading: The dataset is loaded using pandas from a CSV file located in Google Drive.
Random Forest Feature Selection: A Random Forest model is trained on the data, and the importance of each feature is computed. The top 10 features are printed.
Lasso Feature Selection: Lasso regression is used with an alpha of 0.01 to select features with non-zero coefficients. The selected features are printed.
RFE Feature Selection: Recursive Feature Elimination (RFE) is applied to select the top 10 features based on the performance of a Random Forest model.
Combining Selected Features: The top features from Random Forest, Lasso, and RFE are combined to create a set of final selected features.
Correlation Matrix: A heatmap is plotted to visualize the correlations among the selected features.
Saving Processed Data: The selected features along with the target variable Diabetes_012 are saved into a new CSV file.
The code helps in identifying the most important features influencing the target variable and visualizes their relationships.
"""

from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import Lasso
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

data = pd.read_csv('/content/drive/MyDrive/processed_data.csv')

X = data.drop(columns=['Diabetes_012'])
y = data['Diabetes_012']

rf_model = RandomForestClassifier(n_estimators=100)
rf_model.fit(X, y)
feature_importance = pd.Series(rf_model.feature_importances_, index=X.columns).sort_values(ascending=False)

print("\nTop 10 features selected by Random Forest:")
print(feature_importance.head(10))

lasso = Lasso(alpha=0.01)
lasso.fit(X, y)
lasso_selected_features = X.columns[lasso.coef_ != 0]

print("\nTop features selected by Lasso:", lasso_selected_features)

rfe = RFE(estimator=RandomForestClassifier(n_estimators=100), n_features_to_select=10)
X_rfe = rfe.fit_transform(X, y)

selected_features_rfe = X.columns[rfe.support_]
print("\nTop features selected by RFE:", selected_features_rfe)

combined_selected_features = set(feature_importance.head(10).index) | set(lasso_selected_features) | set(selected_features_rfe)
print("\nCombined selected features:", combined_selected_features)

valid_features = [feature for feature in combined_selected_features if feature in X.columns]

sns.heatmap(X[valid_features].corr(), annot=True, cmap='coolwarm')
plt.title("Correlation Matrix of Selected Features")
plt.show()

X_selected = X[valid_features].copy()
X_selected.loc[:, 'Diabetes_012'] = y
X_selected.to_csv('/content/drive/MyDrive/selected_features_data.csv', index=False)
print("Processed data with selected features saved successfully!")

"""The following code implements a machine learning pipeline for diabetes classification. It follows a structured approach to ensure optimal feature selection, model training, and evaluation.

Key Steps:
Data Loading & Preprocessing

The dataset is loaded from Google Drive and reduced to 50,000 rows for efficiency.
The target variable (Diabetes_012) is separated, and features are standardized for better model performance.
Feature Selection

Three techniques are applied:
Random Forest Importance: Identifies the top 10 most influential features.
Lasso Regression: Removes less relevant features based on coefficient values.
Recursive Feature Elimination (RFE): Iteratively selects the most important features.
The best features from all three methods are combined for model training.
Model Training & Evaluation

Three classification models are trained:
Random Forest
Support Vector Machine (SVM)
K-Nearest Neighbors (KNN)
Models are evaluated using multiple metrics:
Accuracy, Precision, Recall, F1-score, and ROC-AUC
Results Visualization

The performance of each model is summarized in a table.
A bar chart provides a visual comparison of the models across different metrics.
This structured approach ensures an efficient and accurate classification of diabetes, leveraging feature selection and multiple machine learning algorithms.
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.feature_selection import RFE
from sklearn.linear_model import Lasso
import matplotlib.pyplot as plt


from google.colab import drive
drive.mount('/content/drive')

# Load dataset
file_path = "/content/drive/MyDrive/project_ML22/diabetes_012_health_indicators_BRFSS2015.csv"
df = pd.read_csv(file_path)

# Reduce dataset size for faster execution (use 50,000 rows)
df = df.sample(n=50000, random_state=42)

# Define features and target variable
X = df.drop(columns=["Diabetes_012"])
y = df["Diabetes_012"]

# Feature Selection
# 1. Random Forest Feature Importance
rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
rf_model.fit(X, y)
feature_importance = pd.Series(rf_model.feature_importances_, index=X.columns).sort_values(ascending=False)
top_rf_features = feature_importance.head(10).index.tolist()

# 2. Lasso Regression
lasso = Lasso(alpha=0.01)
lasso.fit(X, y)
top_lasso_features = X.columns[lasso.coef_ != 0].tolist()

# 3. Recursive Feature Elimination (RFE)
rfe = RFE(estimator=RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1), n_features_to_select=10)
X_rfe = rfe.fit_transform(X, y)
top_rfe_features = X.columns[rfe.support_].tolist()

# Combine selected features
selected_features = list(set(top_rf_features + top_lasso_features + top_rfe_features))
print("Selected Features:", selected_features)

# Update X with selected features
X = X[selected_features]

# Split dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define models
models = {
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),
    "SVM": SVC(probability=True, random_state=42),
    "KNN": KNeighborsClassifier(n_neighbors=5, n_jobs=-1)
}

# Train and evaluate models
results = {}
for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    y_proba = model.predict_proba(X_test_scaled) if hasattr(model, "predict_proba") else None

    results[name] = {
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred, average='weighted'),
        "Recall": recall_score(y_test, y_pred, average='weighted'),
        "F1-score": f1_score(y_test, y_pred, average='weighted'),
        "ROC-AUC": roc_auc_score(y_test, y_proba, multi_class="ovr") if y_proba is not None else "N/A"
    }

# Convert results to DataFrame and display
results_df = pd.DataFrame(results).T
print(results_df)

# Plot results
results_df.drop(columns=["ROC-AUC"], errors='ignore').plot(kind='bar', figsize=(10,6))
plt.title("Model Comparison")
plt.ylabel("Score")
plt.xticks(rotation=0)
plt.legend(loc="lower right")
plt.show()